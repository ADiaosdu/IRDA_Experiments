{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 信息检索与数据挖掘实验一\n",
    "\n",
    "##                      ——布尔查询模型\n",
    "### 数据18.1 201800130083 刁龙飞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立倒排索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据读取\n",
    "def ReadData(text):\n",
    "    data = []\n",
    "    for line in text.readlines():\n",
    "        data.append([eval(line).get('tweetId'),eval(line).get('text')]) #读取使用的tweetId和text两项\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据预处理\n",
    "def DataPreprocess(data_lists): #参数是一个二维列表，每一项表示为[id,text]\n",
    "    file = []    \n",
    "    unused_words_file = open('unused.txt','r')\n",
    "    unused_words = unused_words_file.read()\n",
    "    unused_words_file.close()\n",
    "    unused_words_list = unused_words.split()\n",
    "    for data_list in data_lists: \n",
    "        #全部变成小写\n",
    "        data_list[1] = data_list[1].lower()\n",
    "        # 特殊符号替换：可以保留一部分网址、邮箱等特殊词项\n",
    "        # 删除所有的逗号、括号、其他特殊符号\n",
    "        for ch in '\"#$%&()*+,-—–;<=>[]^_‘{|}~“”\"\"':\n",
    "            data_list[1] = data_list[1].replace(ch,' ')         \n",
    "        for s in ['…','..','...','....','.....','......','. ',' .',': ',\"' \",\" '\",\"''\"]:            \n",
    "            data_list[1] = data_list[1].replace(s,' ')\n",
    "            \n",
    "        # 特殊位置替换\n",
    "        words_list = data_list[1].split()\n",
    "        for j,item in enumerate(words_list):\n",
    "            if item[0] in \"?'./!\":\n",
    "                words_list[j] = item.replace(item[0],'')\n",
    "            if item[-1] in \"?'./!\":\n",
    "                words_list[j] = item.replace(item[-1],'')\n",
    "        data_list[1] = ' '.join(words_list)\n",
    "        t = data_list[1].split(' ')\n",
    "        t.insert(0,data_list[0])\n",
    "        file.append(t)\n",
    "        \n",
    "    # 去除停用词\n",
    "    for i,words in enumerate(file):\n",
    "        for j in unused_words_list:\n",
    "            if j in words:\n",
    "                words.pop(words.index(j))                    \n",
    "    # 词项归一化，暂时不作处理\n",
    "    \n",
    "    # 词干还原 \n",
    "    porter_stemmer = PorterStemmer()  \n",
    "    for i,line in enumerate(file):\n",
    "        for j,word in enumerate(line):\n",
    "            line[j] = porter_stemmer.stem(word)\n",
    "        file[i] = line  \n",
    "          \n",
    "    file.sort() #好像没什么用 原本的tweets就是有序的\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立倒排索引\n",
    "def get_inverted_index(file):\n",
    "    inverted_index = {}\n",
    "    tweet_dicts_list = []\n",
    "    #建立每个tweet的词项字典\n",
    "    for tweet in file:\n",
    "        tweet[0] = eval(tweet[0]) #因为前面要过词干提取，所以那时候id要用字符串\n",
    "        tweet_dict = {}\n",
    "        for i,item in enumerate(tweet):\n",
    "            if i==0:\n",
    "                continue\n",
    "            else:  #删掉空的词项\n",
    "                if item == '':\n",
    "                    continue\n",
    "                tweet_dict[item] = [1,tweet[0]] \n",
    "        tweet_dicts_list.append(tweet_dict)\n",
    "    #对词项字典进行合并\n",
    "    for tweet_dict in tweet_dicts_list:\n",
    "        for key in tweet_dict:    \n",
    "            if key not in inverted_index:\n",
    "                inverted_index[key] = tweet_dict[key]\n",
    "            else:\n",
    "                inverted_index[key].append(tweet_dict[key][1])\n",
    "                inverted_index[key][0] +=1\n",
    "    #对输出的倒排索引排序 根据key的字母序\n",
    "    sorted_inverted_index_keys = sorted(inverted_index.keys())\n",
    "    sorted_inverted_index_list = []\n",
    "    for key in sorted_inverted_index_keys:        \n",
    "        sorted_inverted_index_list.append([key,inverted_index[key]])  \n",
    "    return sorted_inverted_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['28965792812892160', 'House may kill Arizona-style immigration bill, Rep. Rick Rand says: The House is unlikely to pass the \"Ari... http://tinyurl.com/4jrjcdz.'], ['28967095878287360', \"Mourners recall Sarge Shriver's charity, idealism \\n    (AP): AP - R. Sargent Shriver was always an optimist, pio... http://bit.ly/gqMcdG\"], ['28967672074993664', 'Bass Fishing Techniques: 2 Fantastic Tips To Improve Your Casting Skills'], ['28967914417688576', '#Financial Aid | Proper Method Of Getting Financial Aid For Education http://ping.fm/BK0R3 #applying-for-financial-aid financial-aid-essay #'], ['28968479176531969', \"Supreme Court: NASA's intrusive background checks OK http://bit.ly/h2jgy9\"], ['28968581949558787', 'The McDonalds music to fireworks is an all time low.'], ['28969422056071169', \"@alyce Very sweet and quiet, if not polished - Bono & Hansard at Sgt Shriver's funeral 2day: http://youtu.be/Bf14XBbcVZg (when was ...cont'd\"], ['28971749961891840', \"So, Avon&Somerset Police have charged Vincent Tabak with the murder of Jo Yeates. I really hope they're right, otherwise his life is ruined.\"], ['28973080491589632', 'Hawaii Gov Waffles on Obama’s Birth Certificate – Patriot Update http://t.co/1UxYa0r via @AddThis'], ['28974862038994945', 'Ive never retweeted myself but wanted to pass on to @atu2 RT @tommymcgregor: I Want Bono To Sing At My Funeral! http://bit.ly/i0KdEn']]\n"
     ]
    }
   ],
   "source": [
    "tweets_open = open('tweets.txt','r')\n",
    "data_ori = ReadData(tweets_open)\n",
    "tweets_open.close()\n",
    "print(data_ori[:10]) #读取的原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['28965792812892160', 'hous', 'may', 'kill', 'arizona', 'style', 'immigr', 'bill', 'rep', 'rick', 'rand', 'say', 'hous', 'unlik', 'pass', 'the', 'ari', 'http://tinyurlcom/4jrjcdz'], ['28967095878287360', 'mourner', 'recal', 'sarg', \"shriver'\", 'chariti', 'ideal', 'ap', 'ap', 'r', 'sargent', 'shriver', 'alway', 'optimist', 'pio', 'http://bit.ly/gqmcdg'], ['28967672074993664', 'bass', 'fish', 'techniqu', '2', 'fantast', 'tip', 'improv', 'your', 'cast', 'skill'], ['28967914417688576', 'financi', 'aid', 'proper', 'method', 'get', 'financi', 'aid', 'educ', 'http://ping.fm/bk0r3', 'appli', 'for', 'financi', 'aid', 'financi', 'aid', 'essay'], ['28968479176531969', 'suprem', 'court', \"nasa'\", 'intrus', 'background', 'check', 'ok', 'http://bit.ly/h2jgy9'], ['28968581949558787', 'mcdonald', 'music', 'firework', 'all', 'time', 'low'], ['28969422056071169', '@alyc', 'veri', 'sweet', 'quiet', 'if', 'not', 'polish', 'bono', 'hansard', 'sgt', \"shriver'\", 'funer', '2day', 'http://youtu.be/bf14xbbcvzg', 'when', \"cont'd\"], ['28971749961891840', 'so', 'avon', 'somerset', 'polic', 'have', 'charg', 'vincent', 'tabak', 'murder', 'jo', 'yeat', 'i', 'realli', 'hope', \"they'r\", 'right', 'otherwis', 'hi', 'life', 'ruin'], ['28973080491589632', 'hawaii', 'gov', 'waffl', 'obama’', 'birth', 'certif', 'patriot', 'updat', 'http://t.co/1uxya0r', 'via', '@addthi'], ['28974862038994945', 'ive', 'never', 'retweet', 'myself', 'but', 'want', 'pass', 'to', '@atu2', 'rt', '@tommymcgregor', 'i', 'want', 'bono', 'to', 'sing', 'my', 'funer', 'http://bit.ly/i0kden']]\n"
     ]
    }
   ],
   "source": [
    "id_list_file = open('all_text.txt','w',encoding='utf-8')\n",
    "id_list_file.write(str(data_ori))\n",
    "id_list_file.close()\n",
    "data = DataPreprocess(data_ori)\n",
    "print(data[:10]) #预处理后的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['em', [24, 30140006722969600, 301710958420688896, 301835432755331072, 302137452007669761, 302455455777701890, 30280495124193280, 304397288481103872, 305444522333200385, 30637444080607233, 307544274776510464, 309964690166001664, 311966509855547392, 313423107731890176, 33539217275486208, 33840094867623936, 34314860862898178, 34397493319962624, 624249890965581824, 624790327031918593, 624935953241649152, 624937249294168064, 625776810563923968, 626217304729235456, 626430736099373056]], ['email', [22, 29576869535817728, 29898299192385538, 303271814136729600, 303646713581826049, 307894083920207872, 307894599823802369, 307898622173908992, 307902648680599552, 307904263512809475, 307912480133361664, 307914334019936256, 307936073126907907, 307937528521060352, 308261559489163265, 308321307328667648, 308333579899908096, 308367981560356866, 309080413454876672, 310751122170212352, 31771605050855424, 623890778826235904, 625014424483561472]], ['eman', [2, 30067826060435456, 30067834360963073]], ['emanu', [3, 29615199631708161, 297485981488128000, 298460381217710080]], ['emanuel', [104, 29602019660533760, 29602145401569281, 29603137870364672, 29604188728074240, 29604332601090048, 29605381974007809, 29605434390220800, 29606415127547905, 29606637102702593, 29608545469075456, 29609861318705153, 29610484609060864, 29610644344934400, 29610756999749632, 29610900130373632, 29610928903299072, 29612419269525504, 29616051847176192, 29616966620676096, 29617833742696448, 29620982062776320, 29621072722661376, 29621073565712384, 29622817754447872, 29628012471255040, 29630798625767425, 29631260305391616, 29634699332685824, 29635195187499009, 29637571675951104, 29642099909468160, 29642946265485312, 29649212366917632, 29656497839415296, 29661336879239169, 29671069405151232, 29676981104680960, 29688185005019136, 29694510963367936, 29708217516826624, 29714838418628608, 29716088212168705, 29720077087547392, 29726337899962368, 297323741589929984, 29736787173707776, 29753054228119553, 29816837160046592, 29873701256167424, 29891046645170177, 29894770717368320, 29914525234896897, 29938994254979072, 29942734030635008, 29944405947322368, 29950724490133505, 29963957057880067, 29966764259418112, 29979785870450690, 29979787090984960, 29981750247563264, 29986568634634241, 29992277275316225, 29995067955482624, 29996481968611328, 30002622148644866, 30008563728392192, 30015571043033088, 30015603104284672, 30022830624088064, 30097429839740928, 30101682566209536, 30233366276087808, 30305888401104896, 30307254150365184, 30356159202336768, 30429676585353216, 30764349832306689, 30767633527742465, 30768499169169410, 30769192563113984, 30769661930905601, 30769994920890369, 30770321975939072, 30773099582464000, 30777895618088960, 30778510737940482, 30784884444237826, 30786150633308160, 30787730694733825, 30793840872923137, 30794785610530816, 30796220553236482, 30806290825478144, 30826476634902528, 30828837197578241, 30841242736660480, 30846813036617728, 30850469203025920, 30882540759810048, 30933644491100161, 30993728348880896, 31018836186628097, 31185639047172097]], [\"emanuel'\", [11, 29964806458970112, 29968696868864000, 29976628511645697, 29981750247563264, 29988033105240064, 30013954906398720, 30018721376378881, 30252359724572672, 30272609513897984, 30792615909662720, 31072366792544257]], [\"emanuel\\\\'\", [1, 29967133987311617]], ['emanuel’', [3, 29995290803044352, 30764163454214144, 30933644491100161]], ['emarket', [6, 30642168938889216, 30642236995674112, 30642254842433536, 30642412799922177, 30645241832800256, 30650642112450560]], ['embalm', [7, 309818178937180160, 309854719726211073, 310011565741068288, 312092255064322048, 312093702115979264, 312093978956808192, 312742661599686657]]]\n"
     ]
    }
   ],
   "source": [
    "inverted_index = get_inverted_index(data)\n",
    "inverted_index_file = open('inverted_index.txt','w',encoding='utf-8')\n",
    "inverted_index_file.write(str(inverted_index))\n",
    "inverted_index_file.close()\n",
    "print(inverted_index[12010:12020])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据倒排索引建立布尔查询模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读入ID\n",
    "id_open = open('all_text.txt','r',encoding='utf-8')\n",
    "id_data = id_open.read()\n",
    "id_open.close()\n",
    "id_list = []\n",
    "for i in eval(id_data):\n",
    "    id_list.append(eval(i[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#读入倒排索引\\ninverted_index_open = open('inverted_index.txt','r',encoding='utf-8')\\ninverted_index_str = inverted_index_open.read()\\ninverted_index_open.close()\\ninverted_index = eval(inverted_index_str)\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#读入倒排索引\n",
    "inverted_index_open = open('inverted_index.txt','r',encoding='utf-8')\n",
    "inverted_index_str = inverted_index_open.read()\n",
    "inverted_index_open.close()\n",
    "inverted_index = eval(inverted_index_str)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_preprocess(query): #输入为一个字符串 采用大致相同的预处理方式 不考虑停用词\n",
    "    #全部变成小写\n",
    "    query = query.lower()\n",
    "    #整句特殊符号的替换\n",
    "    for ch in '!\"#$%&()*+,-—–;<=>[]^_‘{|}~“”\"\"':\n",
    "        query = query.replace(ch, \" \")   \n",
    "    for s in ['…','...','....','.....','......','. ',' .',': ',\"' \",\" '\",\"''\"]:            \n",
    "        query = query.replace(s,' ')\n",
    "    query_word_list = query.split()     \n",
    "    # 特殊位置替换\n",
    "    for j,item in enumerate(query_word_list):\n",
    "        if item[0] in \"?'./\":\n",
    "            query_word_list[j] = item.replace(item[0],'')\n",
    "        if item[-1] in \"?'./\":\n",
    "            query_word_list[j] = item.replace(item[-1],'')\n",
    "            \n",
    "    # 去除停用词\n",
    "                   \n",
    "    # 词项归一化，暂时不作处理\n",
    "    \n",
    "    # 词干还原\n",
    "    porter_stemmer = PorterStemmer()  \n",
    "    for i,word in enumerate(query_word_list):\n",
    "        query_word_list[i] = porter_stemmer.stem(word) \n",
    "    logical_operator_list = ['and','or']     \n",
    "    for i,word in enumerate(query_word_list):\n",
    "        if word in logical_operator_list:\n",
    "            query_word_1 = ' '.join(query_word_list[:i])\n",
    "            query_word_2 = ' '.join(query_word_list[i+1:])\n",
    "            logical_operator = word\n",
    "            return [logical_operator,query_word_1,query_word_2]\n",
    "        #输出根据输入的不同 分为两种\n",
    "    return [''.join(query_word_list)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'ron', 'birthday']\n",
      "['or', 'not ron', 'birthday']\n",
      "['harripotter']\n"
     ]
    }
   ],
   "source": [
    "#示例\n",
    "print(query_preprocess('ron and birthday'))\n",
    "print(query_preprocess('NOT Ron or birthday'))\n",
    "print(query_preprocess('HARRY POTTER'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_op(list1,list2,op): #求两个有序列表的合并算法 交集或补集\n",
    "    answer = []\n",
    "    if op == 'and':\n",
    "        i = j = 0\n",
    "        while i<len(list1) and j<len(list2):\n",
    "            if list1[i]==list2[j]:\n",
    "                answer.append(list1[i])\n",
    "                i+=1\n",
    "                j+=1\n",
    "            elif list1[i]<list2[j]:\n",
    "                i+=1\n",
    "            elif list1[i]>list2[j]:\n",
    "                j+=1\n",
    "    elif op == 'or':\n",
    "        answer = list1[:] #要新建不要索引\n",
    "        for j in list2:\n",
    "            if j not in answer:\n",
    "                answer.append(j)\n",
    "        answer.sort()   \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 6, 9, 10, 11]\n",
      "[1, 2, 3, 6, 7, 9, 10, 11, 13]\n"
     ]
    }
   ],
   "source": [
    "a = [2,3,6,9,10,11]\n",
    "b = [1,2,6,7,9,10,11,13]\n",
    "print(list_op(a,b,'and'))\n",
    "print(list_op(a,b,'or'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最简单的基于一个单词的查询\n",
    "def basic_query(word,inverted_index):\n",
    "    for index in inverted_index:\n",
    "        if index[0] == word:\n",
    "            return index[1][1:]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#简单的布尔查询\n",
    "def simple_bool_query(query_word_1,query_word_2,logical_operator,inverted_index,all_id): \n",
    "    #字符串可能含有not;and/or;一个很复杂的多重列表；所有tweets的ID 用来求补\n",
    "    result = []\n",
    "    for i in [query_word_1,query_word_2]:\n",
    "        query_word_list = i.split(' ')\n",
    "        if len(query_word_list) == 2 and query_word_list[0]=='not':\n",
    "            word = query_word_list[1]\n",
    "            #从倒排索引中找到这个word对应的索引，然后取补\n",
    "            for index in inverted_index:\n",
    "                if index[0] == word:\n",
    "                    result.append( list(set(all_id) - set(index[1][1:])) )\n",
    "        elif len(query_word_list) == 1:\n",
    "            #从倒排索引中找到这个word对应的索引\n",
    "            word = query_word_list[0]\n",
    "            for index in inverted_index:\n",
    "                if index[0] == word:\n",
    "                    result.append(list(index[1][1:]))\n",
    "        else:\n",
    "            word = ''.join(query_word_list)\n",
    "            for index in inverted_index:\n",
    "                if index[0] == word:\n",
    "                    result.append(list(index[1][1:]))\n",
    "    try:\n",
    "        if logical_operator == 'and':\n",
    "            result_id = list(set(result[0]) & set(result[1])) \n",
    "        #    result_id = list_op(result[0],result[1],'and')\n",
    "        elif logical_operator == 'or':\n",
    "            result_id = list(set(result[0]).union(result[1]))\n",
    "        #    result_id = list_op(result[0],result[1],'or')\n",
    "    except IndexError: #如果是不存在的查询结果 就会出现异常\n",
    "        if result == []: #两个都不存在\n",
    "            return []\n",
    "        else: #一个不存在\n",
    "            return result[0]\n",
    "    return result_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#复杂的布尔查询及查询优化\n",
    "def complex_bool_query(simple_query_list,logical_operator_list,inverted_index,all_id):\n",
    "    if logical_operator_list == ['and']*len(logical_operator_list):#查询优化\n",
    "        optimized_query_list = []        \n",
    "        for i in simple_query_list:\n",
    "            if i[0]=='and':\n",
    "                optimized_query_list.append(basic_query(i[1],inverted_index))\n",
    "                optimized_query_list.append(basic_query(i[2],inverted_index))\n",
    "            elif i[0]=='or':\n",
    "                optimized_query_list.append(simple_bool_query(i[1],i[2],'or',inverted_index,all_id))\n",
    "            else:\n",
    "                optimized_query_list.append(basic_query(i[0],inverted_index))\n",
    "        optimized_query_list.sort(key = lambda x:len(x))\n",
    "        for i in optimized_query_list:\n",
    "            optimized_query_list[0] = list_op(optimized_query_list[0],i,'and')\n",
    "        return optimized_query_list[0]\n",
    "    else:\n",
    "        #首先完成每一个简单查询\n",
    "        temp_query_list = []\n",
    "        for i in simple_query_list:\n",
    "            if len(i) == 3:\n",
    "                temp_query_list.append(simple_bool_query(i[1],i[2],i[0],inverted_index,all_id))\n",
    "            elif len(i) == 1:\n",
    "                temp_query_list.append(basic_query(i[0],inverted_index))     \n",
    "        #从左到右执行逻辑查询\n",
    "        for i,op in enumerate(logical_operator_list):\n",
    "            if i == 0:\n",
    "                query_result_list = list_op(temp_query_list[i],temp_query_list[i+1],op)\n",
    "            else:\n",
    "                query_result_list = list_op(query_result_list,temp_query_list[i+1],op)\n",
    "        return query_result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tweets(qualified_id_list, id_data):\n",
    "    qualified_quary_list = []\n",
    "    for i in eval(id_data):\n",
    "            if eval(i[0]) in qualified_id_list:\n",
    "                qualified_quary_list.append([i[1]])\n",
    "    print('共找到符合条件的结果' + str(len(qualified_quary_list)) + '条')\n",
    "    for i in qualified_quary_list:\n",
    "        print(i)\n",
    "    qualified_quary_list.clear()\n",
    "    return  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " 请输入您的查询语句：hello\n",
      "共找到符合条件的结果15条\n",
      "[\"What Hides In @TacoBell ‘Beef’ http://bit.ly/fv2Fqh guess I ordered the Taco 'meat filling' instead of beef 2nite. Hello @DelTaco my friend!\"]\n",
      "['Hello from LAX..the TSA made my last few moments here horrible, otherwise a super trip. I will be back in a week but only for one night.']\n",
      "['@Eli_Tanning Only for iced Coffee & Tazo Iced Teas. Say hello to Trenta  http://bit.ly/hzZDL5']\n",
      "['Current TV just popped up on radar in a major way.  #Hello! http://huff.to/hF1Ib3 via']\n",
      "['Bye Bye #USAirways - hello #AmericanAirlines: http://t.co/ACXOvGgR  the latest mega merger will create the worlds largest airline']\n",
      "['hello worst nightmare...BBC News - Meteor strike injures hundreds in central Russia http://t.co/gl193lEV']\n",
      "['Chinese cyber-attacks: Hello, Unit 61398 | The Economist http://t.co/TDU6Qq5E3T']\n",
      "['Hello World, Cornell scientists 3D print ears with help from rat tails and cow ears:   Science! A t... http://t.co/eRZyjXVkTP Thank You!']\n",
      "['hello Sherlock']\n",
      "['VIDEO: Mad Men Promo Teases Big \"Affair\" for Season 6: Hello, Pete\\'s sideburns and goodbye, Fat Betty!\\nYes, th... http://t.co/NTJCR4XeDG']\n",
      "['MT @lordsirmies Thomas Friedman is wrong about MOOCs (essay) | Inside Higher Ed http://t.co/IUY71sPvCH &lt; cMOOCs vs xMOOCs, hello?']\n",
      "[\"Hello Facebook Friends,\\nI am so relieved that Mayor Bloomberg's ban on large sodas was dismissed by a judge... http://t.co/k8h9Otb3ld\"]\n",
      "['Higgs boson confirmation boosts physicists to higher energy http://t.co/p3a43r34EY  via @msnbc_science RT Hello God Particle']\n",
      "['Gone Girl - the female noir that puts Fifty Shades of Grey in the shade: Farewell chick lit, hello psychologic... http://t.co/8igZuoIJbW']\n",
      "['RT @Channel4Racing: Ooh, hello sunshine! The scene at @Ascot this morning on #KingGeorge day #GoldenHorn #MorningLine http://t.co/s02aWtaQU0']\n"
     ]
    }
   ],
   "source": [
    "#简单查询\n",
    "query = input(' \\n 请输入您的查询语句：')\n",
    "query_list = query_preprocess(query)\n",
    "qualified_quary_list = []\n",
    "if len(query_list) == 1:   #简单查询\n",
    "    qualified_id_list = basic_query(query_list[0],inverted_index)\n",
    "    print_tweets(qualified_id_list, id_data)    \n",
    "elif len(query_list) == 3:    #简单布尔查询\n",
    "    qualified_id_list = simple_bool_query(query_list[1],query_list[2],query_list[0],inverted_index,id_list)   \n",
    "    print_tweets(qualified_id_list, id_data)\n",
    "else:\n",
    "    print('输入异常！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入高级查询包含的简单查询数量：3\n",
      "请输入第1个简单查询：Ron and birthday\n",
      "请选择第1和2个简单查询之间的逻辑连接词（1:and 2:or)1\n",
      "请输入第2个简单查询：Boys or girls\n",
      "请选择第2和3个简单查询之间的逻辑连接词（1:and 2:or)1\n",
      "请输入第3个简单查询：Happy\n",
      "[['and', 'ron', 'birthday'], ['or', 'boy', 'girl'], ['happi']]\n",
      "['and', 'and']\n",
      "共找到符合条件的结果1条\n",
      "['Happy birthday to my boy Ron Weasley']\n"
     ]
    }
   ],
   "source": [
    "#高级查询\n",
    "bool_query_num = int(input('请输入高级查询包含的简单查询数量：'))\n",
    "simple_query_list = []\n",
    "logical_operator_list = []\n",
    "qualified_complex_id_list = []\n",
    "for i in range(bool_query_num):\n",
    "    simple_query_list.append(query_preprocess(input('请输入第{}个简单查询：'.format(i+1))))\n",
    "    if i < bool_query_num-1:\n",
    "        x = input('请选择第{}和{}个简单查询之间的逻辑连接词（1:and 2:or)'.format(i+1,i+2))\n",
    "        if x=='1':\n",
    "            logical_operator_list.append('and')\n",
    "        elif x=='2':\n",
    "            logical_operator_list.append('or')\n",
    "        else:\n",
    "            print('输入错误，默认为\"and\"')\n",
    "            logical_operator_list.append('and')    \n",
    "print(simple_query_list)\n",
    "print(logical_operator_list)\n",
    "qualified_complex_id_list = complex_bool_query(simple_query_list,logical_operator_list,inverted_index,id_list)\n",
    "print_tweets(qualified_complex_id_list, id_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
