{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据读取\n",
    "def ReadData(text):\n",
    "    data = []\n",
    "    for line in text.readlines():\n",
    "        data.append([eval(line).get('tweetId'),eval(line).get('text')]) #读取使用的tweetId和text两项\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据预处理\n",
    "def DataPreprocess(data_lists): #参数是一个二维列表，每一项表示为[id,text]\n",
    "    file = []    \n",
    "    unused_words_file = open('unused.txt','r')\n",
    "    unused_words = unused_words_file.read()\n",
    "    unused_words_file.close()\n",
    "    unused_words_list = unused_words.split()\n",
    "    for data_list in data_lists: \n",
    "        #全部变成小写\n",
    "        data_list[1] = data_list[1].lower()\n",
    "        # 特殊符号替换：可以保留一部分网址、邮箱等特殊词项\n",
    "        # 删除所有的逗号、括号、其他特殊符号\n",
    "        for ch in '\"#$%&()*+,-—–;<=>[]^_‘{|}~“”\"\"':\n",
    "            data_list[1] = data_list[1].replace(ch,' ')         \n",
    "        for s in ['…','..','...','....','.....','......','. ',' .',': ',\"' \",\" '\",\"''\"]:            \n",
    "            data_list[1] = data_list[1].replace(s,' ')\n",
    "            \n",
    "        # 特殊位置替换\n",
    "        words_list = data_list[1].split()\n",
    "        for j,item in enumerate(words_list):\n",
    "            if item[0] in \"?'./!\":\n",
    "                words_list[j] = item.replace(item[0],'')\n",
    "            if item[-1] in \"?'./!\":\n",
    "                words_list[j] = item.replace(item[-1],'')\n",
    "        data_list[1] = ' '.join(words_list)\n",
    "        t = data_list[1].split(' ')\n",
    "        t.insert(0,data_list[0])\n",
    "        file.append(t)\n",
    "        \n",
    "    # 去除停用词\n",
    "    for i,words in enumerate(file):\n",
    "        for j in unused_words_list:\n",
    "            if j in words:\n",
    "                words.pop(words.index(j))                    \n",
    "    # 词项归一化，暂时不作处理\n",
    "    \n",
    "    # 词干还原 \n",
    "    porter_stemmer = PorterStemmer()  \n",
    "    for i,line in enumerate(file):\n",
    "        for j,word in enumerate(line):\n",
    "            line[j] = porter_stemmer.stem(word)\n",
    "        file[i] = line  \n",
    "          \n",
    "    file.sort() #好像没什么用 原本的tweets就是有序的\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立倒排索引\n",
    "def get_inverted_index(file):\n",
    "    inverted_index = {}\n",
    "    tweet_dicts_list = []\n",
    "    #建立每个tweet的词项字典\n",
    "    for tweet in file:\n",
    "        tweet[0] = eval(tweet[0]) #因为前面要过词干提取，所以那时候id要用字符串\n",
    "        tweet_dict = {}\n",
    "        for i,item in enumerate(tweet):\n",
    "            if i==0:\n",
    "                continue\n",
    "            else:  #删掉空的词项\n",
    "                if item == '':\n",
    "                    continue\n",
    "                #对于每一个词项，在该文档出现的次数记下来\n",
    "                if item not in tweet_dict.keys():\n",
    "                    tweet_dict[item] = [1,tweet[0]] #因为要动态修改 所以不能用元组\n",
    "                else:\n",
    "                    tweet_dict[item][0] += 1\n",
    "        tweet_dicts_list.append(tweet_dict)\n",
    "    #对词项字典进行合并 这个版本的倒排索引中就不记录频率了\n",
    "    for tweet_dict in tweet_dicts_list:\n",
    "        for key in tweet_dict:\n",
    "            if key not in inverted_index.keys():\n",
    "                inverted_index[key] = []\n",
    "                inverted_index[key].append(tweet_dict[key])\n",
    "            else:\n",
    "                inverted_index[key].append(tweet_dict[key])\n",
    "    #对输出的倒排索引排序 根据key的字母序 字典肯定没办法排序的\n",
    "    sorted_inverted_index_keys = sorted(inverted_index.keys())\n",
    "    sorted_inverted_index_list = []\n",
    "    for key in sorted_inverted_index_keys:        \n",
    "        sorted_inverted_index_list.append([[key,len(inverted_index[key])],inverted_index[key]])  \n",
    "    return sorted_inverted_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['28965792812892160', 'hous', 'may', 'kill', 'arizona', 'style', 'immigr', 'bill', 'rep', 'rick', 'rand', 'say', 'hous', 'unlik', 'pass', 'the', 'ari', 'http://tinyurlcom/4jrjcdz'], ['28967095878287360', 'mourner', 'recal', 'sarg', \"shriver'\", 'chariti', 'ideal', 'ap', 'ap', 'r', 'sargent', 'shriver', 'alway', 'optimist', 'pio', 'http://bit.ly/gqmcdg'], ['28967672074993664', 'bass', 'fish', 'techniqu', '2', 'fantast', 'tip', 'improv', 'your', 'cast', 'skill'], ['28967914417688576', 'financi', 'aid', 'proper', 'method', 'get', 'financi', 'aid', 'educ', 'http://ping.fm/bk0r3', 'appli', 'for', 'financi', 'aid', 'financi', 'aid', 'essay'], ['28968479176531969', 'suprem', 'court', \"nasa'\", 'intrus', 'background', 'check', 'ok', 'http://bit.ly/h2jgy9'], ['28968581949558787', 'mcdonald', 'music', 'firework', 'all', 'time', 'low'], ['28969422056071169', '@alyc', 'veri', 'sweet', 'quiet', 'if', 'not', 'polish', 'bono', 'hansard', 'sgt', \"shriver'\", 'funer', '2day', 'http://youtu.be/bf14xbbcvzg', 'when', \"cont'd\"], ['28971749961891840', 'so', 'avon', 'somerset', 'polic', 'have', 'charg', 'vincent', 'tabak', 'murder', 'jo', 'yeat', 'i', 'realli', 'hope', \"they'r\", 'right', 'otherwis', 'hi', 'life', 'ruin'], ['28973080491589632', 'hawaii', 'gov', 'waffl', 'obama’', 'birth', 'certif', 'patriot', 'updat', 'http://t.co/1uxya0r', 'via', '@addthi'], ['28974862038994945', 'ive', 'never', 'retweet', 'myself', 'but', 'want', 'pass', 'to', '@atu2', 'rt', '@tommymcgregor', 'i', 'want', 'bono', 'to', 'sing', 'my', 'funer', 'http://bit.ly/i0kden'], ['28974904342740992', \"oprah'\", 'famili', 'secret', 'weekli', 'world', 'news', 'weekli', 'world', 'news', 'not', 'wait', 'the', 'big', 'o', 'divulg', 'her', 'litt', 'http://bit.ly/id1qio'], ['28976409057697792', 'iran', 'nuclear', 'talk', 'end', 'no', 'agreement', 'us', 'offici', 'say', 'six', 'power', 'align', 'washington', 'post', 'fox', 'news', 'blog', 'http://bit.ly/e78urg'], ['28976831738683393', 'job', 'realli', \"obama'\", 'focu', 'more', 'job', 'loss', 'unemploy', 'figur', 'seemingli', 'steadi', 'over', 'nine', 'percent', 't', 'http://bit.ly/el4klf'], ['28977078074343425', 'cyber', 'worm', 'turn', 'ani', 'kind', 'comput', 'network', 'manufactur', 'of', 'industri', 'control', 'automat', 'http://bit.ly/gybgew'], ['28977078074343425', 'cyber', 'worm', 'turn', 'ani', 'kind', 'comput', 'network', 'manufactur', 'of', 'industri', 'control', 'automat', 'http://bit.ly/gybgew'], ['28977806142603264', 'jame', 'franco', 'wig', 'out', 'sundanc', 'just', 'jare', 'here', 'blog', 'insid', 'deck', 'out', 'bing', 'bar', 'at', 'the', '2011', 'http://dlvr.it/dtfs9'], ['28978641706684416', 'vincent', 'tabak', 'charg', 'murder', 'joanna', 'yeat', 'http://newzfor.me/?cbqb'], ['28978659108855809', 'bachmann', 'give', 'her', 'own', 'state', 'union', 'rebutt', 'http://huff.to/gmfrxk', 'via', '@huffingtonpost', 'whi', 'whi', 'whi'], ['28978692289994752', 'sun', 'man', 'charg', 'kill', 'jo', 'yeat', 'polic', 'tonight', 'have', 'charg', '32', 'year', 'old', 'vincent', 'tabac', 'with', 'murder', 'http://bit.ly/ievzdg'], ['28978703102902273', 'man', 'charg', 'jo', 'yeater', 'murder', 'www.desimag.co.uk']]\n",
      "[[['em', 24], [[1, 30140006722969600], [1, 301710958420688896], [1, 301835432755331072], [1, 302137452007669761], [2, 302455455777701890], [1, 30280495124193280], [1, 304397288481103872], [1, 305444522333200385], [1, 30637444080607233], [1, 307544274776510464], [1, 309964690166001664], [1, 311966509855547392], [1, 313423107731890176], [1, 33539217275486208], [1, 33840094867623936], [1, 34314860862898178], [1, 34397493319962624], [1, 624249890965581824], [1, 624790327031918593], [1, 624935953241649152], [1, 624937249294168064], [1, 625776810563923968], [1, 626217304729235456], [1, 626430736099373056]]], [['email', 22], [[1, 29576869535817728], [1, 29898299192385538], [1, 303271814136729600], [1, 303646713581826049], [1, 307894083920207872], [1, 307894599823802369], [1, 307898622173908992], [1, 307902648680599552], [1, 307904263512809475], [1, 307912480133361664], [1, 307914334019936256], [1, 307936073126907907], [1, 307937528521060352], [1, 308261559489163265], [1, 308321307328667648], [1, 308333579899908096], [1, 308367981560356866], [1, 309080413454876672], [1, 310751122170212352], [1, 31771605050855424], [1, 623890778826235904], [1, 625014424483561472]]], [['eman', 2], [[1, 30067826060435456], [1, 30067834360963073]]], [['emanu', 3], [[1, 29615199631708161], [1, 297485981488128000], [1, 298460381217710080]]], [['emanuel', 104], [[2, 29602019660533760], [1, 29602145401569281], [1, 29603137870364672], [1, 29604188728074240], [1, 29604332601090048], [1, 29605381974007809], [1, 29605434390220800], [1, 29606415127547905], [1, 29606637102702593], [1, 29608545469075456], [1, 29609861318705153], [1, 29610484609060864], [1, 29610644344934400], [1, 29610756999749632], [1, 29610900130373632], [1, 29610928903299072], [1, 29612419269525504], [1, 29616051847176192], [1, 29616966620676096], [1, 29617833742696448], [1, 29620982062776320], [1, 29621072722661376], [1, 29621073565712384], [1, 29622817754447872], [1, 29628012471255040], [1, 29630798625767425], [1, 29631260305391616], [1, 29634699332685824], [1, 29635195187499009], [1, 29637571675951104], [1, 29642099909468160], [2, 29642946265485312], [2, 29649212366917632], [1, 29656497839415296], [1, 29661336879239169], [1, 29671069405151232], [2, 29676981104680960], [1, 29688185005019136], [1, 29694510963367936], [1, 29708217516826624], [1, 29714838418628608], [1, 29716088212168705], [1, 29720077087547392], [2, 29726337899962368], [1, 297323741589929984], [1, 29736787173707776], [1, 29753054228119553], [1, 29816837160046592], [1, 29873701256167424], [1, 29891046645170177], [1, 29894770717368320], [1, 29914525234896897], [1, 29938994254979072], [1, 29942734030635008], [1, 29944405947322368], [1, 29950724490133505], [2, 29963957057880067], [1, 29966764259418112], [1, 29979785870450690], [1, 29979787090984960], [1, 29981750247563264], [1, 29986568634634241], [2, 29992277275316225], [1, 29995067955482624], [1, 29996481968611328], [1, 30002622148644866], [1, 30008563728392192], [1, 30015571043033088], [1, 30015603104284672], [1, 30022830624088064], [1, 30097429839740928], [1, 30101682566209536], [1, 30233366276087808], [1, 30305888401104896], [1, 30307254150365184], [1, 30356159202336768], [1, 30429676585353216], [1, 30764349832306689], [1, 30767633527742465], [1, 30768499169169410], [1, 30769192563113984], [1, 30769661930905601], [2, 30769994920890369], [1, 30770321975939072], [1, 30773099582464000], [1, 30777895618088960], [1, 30778510737940482], [1, 30784884444237826], [2, 30786150633308160], [1, 30787730694733825], [1, 30793840872923137], [1, 30794785610530816], [1, 30796220553236482], [1, 30806290825478144], [1, 30826476634902528], [2, 30828837197578241], [1, 30841242736660480], [1, 30846813036617728], [1, 30850469203025920], [1, 30882540759810048], [1, 30933644491100161], [1, 30993728348880896], [1, 31018836186628097], [1, 31185639047172097]]]]\n"
     ]
    }
   ],
   "source": [
    "tweets_open = open('tweets.txt','r')\n",
    "data_ori = ReadData(tweets_open)\n",
    "tweets_open.close()\n",
    "N = len(data_ori)\n",
    "id_list_file = open('all_text.txt','w',encoding='utf-8')\n",
    "id_list_file.write(str(data_ori))\n",
    "id_list_file.close()\n",
    "data = DataPreprocess(data_ori)\n",
    "print(data[:20])\n",
    "inverted_index= get_inverted_index(data)\n",
    "inverted_index_file = open('inverted_index.txt','w',encoding='utf-8')\n",
    "inverted_index_file.write(str(inverted_index))\n",
    "inverted_index_file.close()\n",
    "print(inverted_index[12010:12015])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inverted_index = eval(open('inverted_index.txt','r',encoding='utf-8').read())\n",
    "print(inverted_index[12010:12015])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for i in inverted_index:\n",
    "    df.append(i[0][1])\n",
    "df = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_vector(data):\n",
    "    id_dic = {}\n",
    "    for i in data:\n",
    "        id_dic[i[0]] = 0\n",
    "    return id_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index_vector(inverted_index):\n",
    "    inverted_index_dic = {}\n",
    "    for i in inverted_index:\n",
    "        inverted_index_dic[i[0][0]] = 0\n",
    "    return inverted_index_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_log(array):\n",
    "    tf_log_list = []\n",
    "    for i in array:\n",
    "        if i == 0:\n",
    "            tf_log_list.append(0)\n",
    "        else:\n",
    "            tf_log_list.append(1+np.log10(i))\n",
    "    return np.array(tf_log_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def option_check(option):\n",
    "    std_option = ['nlabL','ntp','nc']\n",
    "    if len(option) == 3:\n",
    "        for i in range(3):\n",
    "            if option[i] in std_option[i]:\n",
    "                pass\n",
    "            else:\n",
    "                return False\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL SMART notations\n",
    "def doc_vector(df, term_list, inverted_index, option):#将文档根据倒排索引转化成一个稀疏的向量 有归一化\n",
    "    term_dict = inverted_index_vector(inverted_index)\n",
    "    tf_raw = []\n",
    "    id_list = []\n",
    "    for term in term_list:#对于每个查询词项\n",
    "        #计算term在term_list中的出现次数:tf-raw\n",
    "        if term not in term_dict.keys():\n",
    "            pass #对于所有文档中都没有出现过的词忽略不计\n",
    "        else:\n",
    "            term_dict[term] += 1\n",
    "    for i in inverted_index:\n",
    "        if i[0][0] in term_list:\n",
    "            for j in i[1]:\n",
    "                id_list.append(j[1])\n",
    "    for key in term_dict:\n",
    "        tf_raw.append(term_dict[key])\n",
    "    option_list = [[np.array(tf_raw),tf_log(np.array(tf_raw)),(0.5 + (0.5*np.array(tf_raw)/max(tf_raw))),np.sign(tf_raw),(tf_log(np.array(tf_raw))/tf_log(np.full_like(np.array(tf_raw),np.mean(tf_raw),dtype = 'float')))],\n",
    "                  [1,tf_log(N/np.array(df)),np.maximum(0,tf_log((N - np.array(df))/np.array(df)))],\n",
    "                  [1,1 / np.linalg.norm(np.array(tf_raw))]]\n",
    "\n",
    "    #option是一个字符串\n",
    "    std_option = ['nlabL','ntp','nc']\n",
    "    result = np.ones_like(tf_raw)\n",
    "    for i in range(3):\n",
    "        result =result * np.array(option_list[i][std_option[i].find(option[i])])\n",
    "    return result,id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#时间复杂度略高\n",
    "def cosine_score(df, op_q,op_d, query, document_list, inverted_index): #用预处理后的list\n",
    "    scores = []#存放每篇文档的得分\n",
    "    query_vector,id_list = doc_vector(df, query, inverted_index, op_q)\n",
    "    for i in document_list:\n",
    "        if i[0] in id_list:\n",
    "            document_vector,unuse = doc_vector(df, i[1:], inverted_index, op_d)\n",
    "            scores.append([i[0],sum(query_vector*document_vector)/len(i)])    \n",
    "    return sorted(scores,key = lambda x:x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入查询语句：a big house\n",
      "请输入文档处理模式：anc\n",
      "请输入查询处理模式：bnn\n",
      "共找到符合条件的结果50条，评分最高的前10条如下\n",
      "house may kill arizona style immigration bill rep rick rand says the house is unlikely to pass the ari http://tinyurlcom/4jrjcdz\n",
      "oprah's family secret weekly world news the weekly world news will not wait for the big o to divulge her litt http://bit.ly/id1qio\n",
      "new easy recipe ranch house casserole http://bit.ly/htangc recipes\n",
      "producers guild gives the king's speech the big win http://st.ep2.tv/fbhsx2 film\n",
      "eric cantor addresses birther issue washington the new republican house majority leader says he doesn't thi http://twurl.nl/lr74ji\n",
      "dtn world news cantor i believe obama is a u.s citizen the new republican house majority leader says he does http://bit.ly/g6n09o\n",
      "understatement 8/5 runs in race 8 at big a for a 50g tag 6 year old horse with class but the tag sends mixed signals willing to lose him\n",
      "chinese pianist plays anti american tune at white house the blaze http://goo.gl/zu4jj\n",
      "@apoloohno if that horse races bet big but make sure the amount you bet is all 8's\n",
      "oprah winfrey says she's revealing a big family secret tomorrow on the oprah winfrey show http://bit.ly/hbtfpn\n"
     ]
    }
   ],
   "source": [
    "query = input('请输入查询语句：')\n",
    "query_list = DataPreprocess([['0',query]])[0][1:]\n",
    "option_doc = input('请输入文档处理模式：')\n",
    "while not option_check(option_doc):\n",
    "    option_doc = input('输入错误，请重新输入：')\n",
    "option_query = input('请输入查询处理模式：')\n",
    "while not option_check(option_query):\n",
    "    option_query = input('输入错误，请重新输入：')\n",
    "qualified_quary_list = []\n",
    "id_list = []\n",
    "score_list = cosine_score(df, option_query, option_doc, query_list, data[:2000], inverted_index)\n",
    "for i in score_list:\n",
    "    id_list.append(str(i[0]))\n",
    "for i in data_ori:\n",
    "    if i[0] in id_list:\n",
    "        qualified_quary_list.append(i[1])\n",
    "print('共找到符合条件的结果{}条，评分最高的前10条如下'.format(len(qualified_quary_list)))\n",
    "for i in qualified_quary_list[:10]: #K\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
